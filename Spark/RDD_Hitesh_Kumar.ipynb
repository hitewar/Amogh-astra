{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1.Create a RDD using parallelize method and implement map and flatMap transformations?\n",
    "RDD=sc.textFile(\"/home/affine/Desktop/Affine/07PySpark/Assignment01/Orders.txt\")\n",
    "words=sc.parallelize(RDD.collect())\n",
    "words01=words.map(lambda line:line.split(\",\"))\n",
    "words02=words01.flatMap(lambda line:line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A', '100 '],\n",
       " ['2', 'B', '100'],\n",
       " ['1', 'C', '250'],\n",
       " ['1', 'D', '350'],\n",
       " ['3', 'E', '120'],\n",
       " ['5', 'F', '130'],\n",
       " ['4', 'G', '230'],\n",
       " ['5', 'H', '145']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD=sc.textFile(\"/home/affine/Desktop/Affine/07PySpark/Assignment01/Orders01\")\n",
    "headers=RDD.first()\n",
    "RDD_data= RDD.filter(lambda line:line!=headers)\n",
    "RDD_data01 =RDD_data.map(lambda line:line.split(\",\"))\n",
    "RDD_data01.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 'A', 100],\n",
       " [4, 'B', 100],\n",
       " [2, 'C', 250],\n",
       " [2, 'D', 350],\n",
       " [6, 'E', 120],\n",
       " [10, 'F', 130],\n",
       " [8, 'G', 230],\n",
       " [10, 'H', 145]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dummy_func(x):\n",
    "    return x *2\n",
    "RDD_data02= RDD_data01.map(lambda line:[dummy_func(int(line[0])),line[1],int(line[2]) ]) \n",
    "RDD_data02.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.19999999990273e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "startTimeQuery = time.clock()\n",
    "\n",
    "endTimeQuery = time.clock()\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(runTimeQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['106', 'F', '26', '18000'], ['107', 'G', '27', '20000'], ['108', 'H', '28', '9000'], ['109', 'I', '29', '24000'], ['110', 'J', '30', '22000']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Create a RDD from external dataset employee.txt and count the number of \n",
    "#   employees age greater than 25?\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "file2=sc.textFile(\"/home/affine/Desktop/Affine/07PySpark/Assignment01/Employee.txt\")\n",
    "header=file2.first()\n",
    "file_data=file2.filter(lambda line:line!=header)\n",
    "temp_var=file_data.map(lambda k:k.split(\",\"))\n",
    "PairERdd=temp_var.filter(lambda x:int(x[2])>25)\n",
    "print(PairERdd.collect())\n",
    "PairERdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[476] at RDD at PythonRDD.scala:48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['A', '21', '8000'],\n",
       " ['B', '22', '14000'],\n",
       " ['C', '23', '7000'],\n",
       " ['D', '24', '12000'],\n",
       " ['E', '25', '16000'],\n",
       " ['F', '26', '18000'],\n",
       " ['H', '28', '9000']]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.create a pair rdd from external dataset Employee.txt with Id as key and rest as values and\n",
    "#find out employees whose salary lesser than 20000?\n",
    "Pair_rdd= temp_var.map(lambda x:(x[0],x[1:]))\n",
    "print(Pair_rdd.values())\n",
    "Pair_rdd.values().filter(lambda x:int(x[2])<20000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'1': 3, '2': 1, '3': 1, '4': 1, '5': 2})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.create a pair rdd from external dataset Oredrs.txt with OrderID as \n",
    "#  key and orderamount as value and count number of unique id's?\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "file4=sc.textFile(\"/home/affine/Desktop/Affine/07PySpark/Assignment01/Orders.txt\")\n",
    "\n",
    "header = file4.first()\n",
    "file_data = file4.filter(lambda line: line != header)\n",
    "temp_var1=file_data.map(lambda k:k.split(\",\"))\n",
    "temp_var1.collect()\n",
    "Pair_rdd=temp_var1.map(lambda x:(x[0],x[2]))\n",
    "Pair_rdd.countByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 700), ('4', '230'), ('2', '100'), ('3', '120'), ('5', 275)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.From OrderPairRdd(pairrdd created from question 3) find out number of different orders for \n",
    "# each OrderID and their sum. (Hint: use pairrdd transformations)\n",
    "AmountPerID=Pair_rdd.reduceByKey(lambda x,y:int(x)+int(y))\n",
    "AmountPerID.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['106', 'F', '26', 19800],\n",
       " ['107', 'G', '27', 22000],\n",
       " ['108', 'H', '28', 9900],\n",
       " ['109', 'I', '29', 26400],\n",
       " ['110', 'J', '30', 24200]]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.From Employee PairRdd(pairRdd created from question2) give a 10% hike for \n",
    "#employees whose age is greater than 25.        \n",
    "PairERdd.map(lambda x:[x[0],x[1],x[2],round(int(x[3]) * 1.1)]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduceByKey() missing 1 required positional argument: 'func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-a6061fae41c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPair_rdd01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPair_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mPair_rdd01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: reduceByKey() missing 1 required positional argument: 'func'"
     ]
    }
   ],
   "source": [
    "#7.From OrderPairRdd find OrderID which ordered most overall?\n",
    "header=Pair_rdd.first()\n",
    "Pair_rdd01 = Pair_rdd.filter(lambda line: line!=header)\n",
    "Pair_rdd01.reduceByKey()s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8.Give an example of imlementing flatMapValues \n",
    "# Transformation by using Employee pairRdd or OrdersRdd?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Palindrome\n"
     ]
    }
   ],
   "source": [
    "#9.write a python function for palindrome.\n",
    "def palind(string):                   ##For input is a string\n",
    "    b=list(map(str,string))\n",
    "    b.reverse()\n",
    "    c=''.join(b)\n",
    "    if(string==c):\n",
    "        print(\"It is Palindrome\")\n",
    "    else:\n",
    "        print(\"It is not Palindrome\")\n",
    "    #num=str(num)                       ##For input is a Number\n",
    "    #b=list(map(str,num))\n",
    "    #b.reverse()\n",
    "    #c=''.join(b)\n",
    "    #print(num)\n",
    "    #print(c)\n",
    "    #if(int(num)==int(c)):\n",
    "    #    print(\"It is Palindrome\")\n",
    "    #else:\n",
    "     #   print(\"It is not Palindrome\")\n",
    "            \n",
    "palind(\"PALINDROMEMORDNILAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
